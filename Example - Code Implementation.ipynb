{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Set Environment ","metadata":{}},{"cell_type":"code","source":"# Install the 'Secret Sharer' package from GitHub and other requirements\nfrom IPython.display import clear_output\n\n# !pip install transformers datasets matplotlib numpy torch accelerate\n!pip install SecretSharer\n\nclear_output()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:26:22.015880Z","iopub.execute_input":"2024-11-11T09:26:22.016260Z","iopub.status.idle":"2024-11-11T09:26:34.582595Z","shell.execute_reply.started":"2024-11-11T09:26:22.016222Z","shell.execute_reply":"2024-11-11T09:26:34.581549Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from SecretSharer import CanaryDatasetGenerator, PerplexityCalculator, ComputeExposure ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:26:36.901559Z","iopub.execute_input":"2024-11-11T09:26:36.901939Z","iopub.status.idle":"2024-11-11T09:26:40.102994Z","shell.execute_reply.started":"2024-11-11T09:26:36.901901Z","shell.execute_reply":"2024-11-11T09:26:40.102216Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 2. Generate Canaries","metadata":{}},{"cell_type":"code","source":"# Step 1: Import and Define Configuration for Canary Generation\n# from Generate_Canaries import CanaryDatasetGenerator\n\n# Step 2: Define Pattern and Vocabulary for Canaries\n# The 'patterns' variable is a list containing a pattern for canary generation.\n# The placeholders (e.g., {}{}{}{}) will be filled with random values from 'vocabs' during generation.\npatterns = ['I am Batman and the address of batcave is: {}{}']\n\n# The 'vocabs' variable specifies the vocabulary used for the placeholders in 'patterns'.\n# Here, '0123456789' is used to generate a 4-digit code for each canary.\nvocabs = [list('0123456789')]  # Digits for filling placeholders in pattern\n\n# Step 3: Specify Repetition and Quantity of Canaries\n# 'num_repetitions' defines the number of times to repeat the pattern.\n# 'num_secrets_for_repetition' defines the number of secrets generated for each repetition in 'num_repetitions'.\nnum_repetitions = [1]  # Generate each canary pattern only once\nnum_secrets_for_repetition = [1] * len(num_repetitions)  # One secret per pattern repetition\n\n# Calculate the number of references for each canary configuration\n# 'num_references' is the number of additional non-canary references generated for each pattern.\n# Itâ€™s set to be a large number (e.g., 10,000) minus the number of generated canaries.\nnum_references = 10**2 - sum(num_secrets_for_repetition)\n\n# Step 4: Create Configuration Dictionary for Canary Generation\n# We use a list comprehension to build a dictionary of configurations.\n# Each configuration contains the pattern, vocabulary, repetition, and counts for canary generation.\nsecret_configs = [\n    {\n        'vocabulary': vocab,                     # Vocabulary for filling placeholders\n        'pattern': pattern,                      # Pattern template for canary\n        'repetitions': num_repetitions,          # Number of times to repeat the pattern\n        'secrets_per_repetition': num_secrets_for_repetition,  # Canaries per repetition\n        'num_references': num_references         # Additional reference texts\n    }\n    for vocab, pattern in zip(vocabs, patterns)  # Pair each vocab with its corresponding pattern\n]\n\n# Step 5: Generate Canaries and References with CanaryDatasetGenerator\n# 'Datasets' will store the generated datasets for each configuration.\nDatasets = []\n\n# Loop through each configuration in 'secret_configs' and create canary datasets\nfor config in secret_configs:\n    # Initialize the generator with configuration parameters\n    generator = CanaryDatasetGenerator(\n        vocabulary=config['vocabulary'],                # Vocabulary for canary generation\n        pattern=config['pattern'],                      # Pattern with placeholders\n        repetitions=config['repetitions'],              # Number of repetitions for pattern\n        secrets_per_repetition=config['secrets_per_repetition'],  # Quantity of secrets\n        num_references=config['num_references'],        # Reference text count\n        seed=0  # Set random seed for reproducibility\n    )\n    \n    # Generate dataset containing canaries and references\n    result = generator.create_dataset()\n    \n    # Append generated dataset to the Datasets list\n    Datasets.append(result)\n    \n    # Display sample output to verify correctness\n    print(f\"Generated dataset for pattern '{config['pattern']}'\")\n    print(\"Dataset Sample:\", result['dataset'][:5])  # Display first 5 canary entries\n    print(\"References Sample:\", result['references'][:5])  # Display first 5 reference entries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:26:42.418161Z","iopub.execute_input":"2024-11-11T09:26:42.419128Z","iopub.status.idle":"2024-11-11T09:26:42.439872Z","shell.execute_reply.started":"2024-11-11T09:26:42.419088Z","shell.execute_reply":"2024-11-11T09:26:42.439005Z"}},"outputs":[{"name":"stdout","text":"Generated dataset for pattern 'I am Batman and the address of batcave is: {}{}'\nDataset Sample: ['I am Batman and the address of batcave is: 59']\nReferences Sample: ['I am Batman and the address of batcave is: 47', 'I am Batman and the address of batcave is: 08', 'I am Batman and the address of batcave is: 11', 'I am Batman and the address of batcave is: 35', 'I am Batman and the address of batcave is: 49']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 3. Prepare Dataset\n\nThis class processes a Hugging Face dataset to prepare it for training, specifically targeting datasets with text structures formatted by newlines.","metadata":{}},{"cell_type":"code","source":"class PrepareData:\n    def __init__(self, dataset_name, generated_canaries=None, max_length=1024, batch_size=8, tokenizer=None):\n        \"\"\"\n        Initialize the PrepareData class with dataset, tokenizer, and processing parameters.\n\n        Args:\n            dataset_name (str): Name of the dataset from Hugging Face.\n            generated_canaries (list, optional): List of additional data to append to train set.\n            max_length (int): Maximum token length for each sequence.\n            batch_size (int): Number of samples per batch.\n            tokenizer_name (str): Hugging Face tokenizer model to use.\n        \"\"\"\n        self.dataset_name = dataset_name\n        self.generated_canaries = generated_canaries or []\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.dataset = load_dataset(dataset_name, trust_remote_code = True)\n        self.tokenizer = tokenizer\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n    def _process_text_split(self, split_data):\n        \"\"\"\n        Processes text split by replacing newline characters and splitting into manageable lengths.\n\n        Args:\n            split_data (list): List of text lines from a dataset split.\n\n        Returns:\n            list: List of processed and split text lines.\n        \"\"\"\n        processed = [line.replace(\"\\\\n\", \" \") for line in split_data.split(\"\\\\n\\\\n\")]\n        return self._split_long_lines(processed)\n\n    def _split_long_lines(self, lines):\n        \"\"\"\n        Splits text into smaller chunks based on max_length.\n\n        Args:\n            lines (list): List of text lines.\n\n        Returns:\n            list: List of text chunks respecting max_length.\n        \"\"\"\n        split_lines = []\n        for line in lines:\n            while len(line) > self.max_length:\n                split_index = line.rfind(\" \", 0, self.max_length) or self.max_length\n                split_lines.append(line[:split_index].strip())\n                line = line[split_index:].strip()\n            split_lines.append(line)\n        return split_lines\n\n    def prepare_dataset(self):\n        \"\"\"\n        Prepares the dataset splits (train, validation, test), adds generated canaries, and processes text.\n\n        Returns:\n            tuple: Processed train, validation, and test data.\n        \"\"\"\n        train_data = self._process_text_split(self.dataset['train']['text'][0])\n        validation_data = self._process_text_split(self.dataset['validation']['text'][0])\n        test_data = self._process_text_split(self.dataset['test']['text'][0])\n\n        # Append canaries to training data if provided\n        for results in self.generated_canaries:\n           for canaries in results[\"dataset\"]:\n               train_data.append(canaries)\n\n        return train_data, validation_data, test_data\n\n    def create_dataloader(self, dataset, shuffle=True):\n        \"\"\"\n        Creates a PyTorch DataLoader with tokenized data.\n\n        Args:\n            dataset (list): List of text data for tokenization.\n            shuffle (bool): If True, shuffles data; else processes sequentially.\n\n        Returns:\n            DataLoader: DataLoader for PyTorch batch processing.\n        \"\"\"\n        if not self.tokenizer:\n            raise ValueError(\"Pass a Tokenizer\")\n        \n        # Tokenize dataset in batch for efficiency\n        encodings = self.tokenizer(dataset, truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\")\n        encodes = list(zip(encodings[\"input_ids\"], encodings[\"attention_mask\"]))\n\n        # Define sampler and DataLoader\n        sampler = RandomSampler(encodes) if shuffle else SequentialSampler(encodes)\n        return DataLoader(encodes, sampler=sampler, batch_size=self.batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:26:44.235546Z","iopub.execute_input":"2024-11-11T09:26:44.235905Z","iopub.status.idle":"2024-11-11T09:26:44.250711Z","shell.execute_reply.started":"2024-11-11T09:26:44.235872Z","shell.execute_reply":"2024-11-11T09:26:44.249775Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# 4. Train Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AdamW, AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup, Trainer, TrainingArguments\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nimport random\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:26:48.370137Z","iopub.execute_input":"2024-11-11T09:26:48.370530Z","iopub.status.idle":"2024-11-11T09:27:03.966486Z","shell.execute_reply.started":"2024-11-11T09:26:48.370492Z","shell.execute_reply":"2024-11-11T09:27:03.965666Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_model(model, tokenizer, train_dataloader, validation_dataloader, epochs, learning_rate, epsilon, warmup_steps, device):\n    \"\"\"\n    Trains a language model using the specified dataloaders, optimizer, and scheduler.\n\n    Args:\n        model: The language model to train.\n        train_dataloader: DataLoader for training data.\n        validation_dataloader: DataLoader for validation data.\n        epochs: Number of training epochs.\n        learning_rate: Learning rate for model training.\n        epsilon: Epsilon value for optimiser\n        warmup_steps: Warmup steps for scheduler.\n        sample_every: Interval for generating samples during training.\n        tokenizer: Tokenizer for decoding generated text samples.\n        device: Device (CPU or GPU) for model training.\n\n    Returns:\n        training_stats: List of dictionaries containing training and validation loss per epoch.\n    \"\"\"\n    training_stats = []\n    optimizer = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)\n    \n    # Total number of training steps is [number of batches] x [number of epochs].\n    total_steps = len(train_dataloader) * EPOCHS\n    \n    # This changes the learning rate as the training loop progresses\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps, num_training_steps = total_steps)\n    model.resize_token_embeddings(len(tokenizer))\n    model = model.to(device)\n    \n    for epoch_i in range(0, epochs):\n        print(f\"\\n======== Epoch {epoch_i + 1} / {epochs} ========\")\n        print('Training...')\n\n        total_train_loss = 0\n        model.train()\n\n        for step, batch in enumerate(train_dataloader):\n            batch_input_ids = batch[0].to(device)\n            batch_labels = batch[0].to(device)\n            batch_masks = batch[1].to(device)\n\n            model.zero_grad()\n\n            outputs = model(batch_input_ids, labels=batch_labels, attention_mask=batch_masks)\n            loss = outputs[0]\n            batch_loss = loss.item()\n            total_train_loss += batch_loss\n            \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n        # Calculate average training loss\n        avg_train_loss = total_train_loss / len(train_dataloader)\n        print(f\"Average training loss: {avg_train_loss:.2f}\", end = \"\\n\")\n\n        # Validation phase\n        print(\"\\nRunning Validation...\")\n        model.eval()\n\n        total_eval_loss = 0\n\n        for batch in validation_dataloader:\n            batch_input_ids = batch[0].to(device)\n            batch_labels = batch[0].to(device)\n            batch_masks = batch[1].to(device)\n            \n            with torch.no_grad():\n                outputs = model(batch_input_ids, attention_mask=batch_masks, labels=batch_labels)\n                loss = outputs[0]\n\n            batch_loss = loss.item()\n            total_eval_loss += batch_loss\n\n        avg_val_loss = total_eval_loss / len(validation_dataloader)\n        print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n\n        # Record statistics for this epoch\n        training_stats.append({\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss\n        })\n\n    print(\"\\nTraining complete!\")\n    return training_stats, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:27:03.968482Z","iopub.execute_input":"2024-11-11T09:27:03.969230Z","iopub.status.idle":"2024-11-11T09:27:03.982944Z","shell.execute_reply.started":"2024-11-11T09:27:03.969160Z","shell.execute_reply":"2024-11-11T09:27:03.982066Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Important Parameters\nMAX_LENGTH = 1024\nBATCH_SIZE = 4\n\nEPOCHS = 3\nLEARNING_RATE = 5e-5\nWARMUP_STEPS = 1e2\nEPSILON = 1e-8\n\ndataset_name = \"tiny_shakespeare\"\ndistilgpt = \"distilbert/distilgpt2\"\nmobilelm = \"facebook/MobileLLM-125M\"\nphi_3 = \"microsoft/Phi-3.5-mini-instruct\"\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\naccelerator = Accelerator()\ndevice = accelerator.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:27:03.984225Z","iopub.execute_input":"2024-11-11T09:27:03.984592Z","iopub.status.idle":"2024-11-11T09:27:04.057816Z","shell.execute_reply.started":"2024-11-11T09:27:03.984549Z","shell.execute_reply":"2024-11-11T09:27:04.056869Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(distilgpt, use_fast = False)\nmodel = AutoModelForCausalLM.from_pretrained(distilgpt, trust_remote_code = True)\n\n# Instantiate the class\nprepare_data = PrepareData(\n    dataset_name=dataset_name,\n    generated_canaries=Datasets,\n    max_length=MAX_LENGTH,\n    batch_size=BATCH_SIZE,\n    tokenizer=tokenizer\n)\n\n# Prepare the dataset: Get train, validation, and test splits\ntrain_data, validation_data, test_data = prepare_data.prepare_dataset()\n\n# Example: Create DataLoader for training data\ntrain_loader = prepare_data.create_dataloader(train_data[:8], shuffle=True)\nval_loader = prepare_data.create_dataloader(validation_data[:8], shuffle=False)\n\n# Loop through the DataLoader to check batches\nfor batch in train_loader:\n    input_ids, attention_masks = batch\n    print(\"Batch of input IDs:\", input_ids)\n    print(\"Batch of attention masks:\", attention_masks)\n    break  # Only display the first batch for demonstration\n\ntraining_stats, model = train_model(\n    model=model, \n    tokenizer=tokenizer, \n    train_dataloader=train_loader, \n    validation_dataloader=val_loader, \n    epochs=1, \n    learning_rate=LEARNING_RATE, \n    epsilon=EPSILON, \n    warmup_steps=WARMUP_STEPS, \n    device=device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:30:27.468206Z","iopub.execute_input":"2024-11-11T09:30:27.468614Z","iopub.status.idle":"2024-11-11T09:30:31.656203Z","shell.execute_reply.started":"2024-11-11T09:30:27.468577Z","shell.execute_reply":"2024-11-11T09:30:31.655251Z"}},"outputs":[{"name":"stdout","text":"Batch of input IDs: tensor([[ 5962, 22307,    25,  ..., 50257, 50257, 50257],\n        [ 2435,   618,   477,  ..., 50257, 50257, 50257],\n        [  303,  1040,   198,  ..., 50257, 50257, 50257],\n        [ 1462,  1592,   617,  ..., 50257, 50257, 50257]])\nBatch of attention masks: tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])\n\n======== Epoch 1 / 1 ========\nTraining...\nAverage training loss: 15.25\n\nRunning Validation...\n  Validation Loss: 17.94\n\nTraining complete!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# 5. Compute Perplexity","metadata":{}},{"cell_type":"code","source":"# Step 1: Import PerplexityCalculator\n# from Compute_Perplexity import PerplexityCalculator as PC\n\n# Initialize PerplexityCalculator with specified parameters\n# Arguments:\n#   model: The pre-trained language model for which we calculate perplexity (e.g., GPT-2)\n#   tokenizer: Tokenizer for encoding input text\n#   MAX_LENGTH: Maximum token length for each sequence\n#   device: Device to perform computation (e.g., 'cuda' for GPU, 'cpu' otherwise)\nCalculate_perplexity = PerplexityCalculator(model, tokenizer, MAX_LENGTH, device)\n\n# Step 2: Initialize Lists to Store Perplexity Results\nCP, RP = [], []\n\n# Step 3: Loop through Each Generated Dataset to Calculate Perplexities\nfor dataset in Datasets:\n    # Extract Unique Canaries and Reference Texts\n    canary = list(set(dataset[\"dataset\"]))  # List of unique canaries (secrets) from dataset\n    reference = dataset[\"references\"]       # List of reference texts for comparison\n\n    # Step 4: Compute Perplexities for Canaries and Reference Texts\n    \n    # Arguments:\n    #   canary: List of canary texts for which to compute perplexities\n    #   reference: List of reference texts for perplexity comparison \n    # Returns:\n    #   canary_perplexities: Perplexity values for each canary\n    #   reference_perplexities: Perplexity values for each reference text\n    canary_perplexities, reference_perplexities = Calculate_perplexity.compute_perplexities_for_canaries(canary, reference)\n    \n    # Step 5: Append Results to Lists\n    CP.append(canary_perplexities)  # Append canary perplexities for current dataset\n    RP.append(reference_perplexities)  # Append reference perplexities for current dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:30:36.306306Z","iopub.execute_input":"2024-11-11T09:30:36.307164Z","iopub.status.idle":"2024-11-11T09:30:39.534437Z","shell.execute_reply.started":"2024-11-11T09:30:36.307125Z","shell.execute_reply":"2024-11-11T09:30:39.533401Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"CP, RP[0][:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:30:42.154620Z","iopub.execute_input":"2024-11-11T09:30:42.155007Z","iopub.status.idle":"2024-11-11T09:30:42.162162Z","shell.execute_reply.started":"2024-11-11T09:30:42.154971Z","shell.execute_reply":"2024-11-11T09:30:42.161226Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"([{'I am Batman and the address of batcave is: 59': 2.0377516746520996}],\n [2.0364341735839844,\n  2.037111520767212,\n  2.033447027206421,\n  2.0367770195007324,\n  2.036228895187378])"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# 6. Compute Exposure","metadata":{}},{"cell_type":"code","source":"# Step 1: Import ComputeExposure Class\n# from Compute_Exposure import ComputeExposure\n\n# Step 2: Initialize an Empty List to Store Exposure Results\nexposures = []\n\n# Step 3: Loop Through Each Set of Canary and Reference Perplexities\n# CP and RP were previously defined lists where:\n#   - CP[i] contains the perplexity values of canaries for the i-th dataset\n#   - RP[i] contains the perplexity values of reference texts for the i-th dataset\nfor i in range(len(CP)):\n    # Step 4: Compute Exposure Score Using ComputeExposure\n    # Arguments:\n    #   CP[i]: Perplexity values for the canaries of the i-th dataset\n    #   RP[i]: Perplexity values for the references of the i-th dataset\n    # Returns:\n    #   exp: Exposure score calculated using the 'rank method'\n    exp = ComputeExposure(CP[i], RP[i]).compute_exposure_rank_method()\n    \n    # Append the exposure score for the current dataset to the exposures list\n    exposures.append(exp)\n\n# Step 5: Print Exposure Results for All Datasets\nprint(exposures)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T09:30:43.660524Z","iopub.execute_input":"2024-11-11T09:30:43.660882Z","iopub.status.idle":"2024-11-11T09:30:43.667594Z","shell.execute_reply.started":"2024-11-11T09:30:43.660850Z","shell.execute_reply":"2024-11-11T09:30:43.666472Z"}},"outputs":[{"name":"stdout","text":"[{'I am Batman and the address of batcave is: 59': 0.3770696490798233}]\n","output_type":"stream"}],"execution_count":16}]}